{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e9d4ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataclass initiation average time over 1000000 samples: 0.77 µs\n",
      "Pydantic initiation average time over 1000000 samples: 2.01 µs\n",
      "Msgspec initiation average time over 1000000 samples: 0.19 µs\n"
     ]
    }
   ],
   "source": [
    "from benchmark_dataclass import instantiate_dataclass, encode_dataclass, decode_dataclass\n",
    "from benchmark_pydantic import instantiate_pydantic, encode_pydantic, decode_pydantic\n",
    "from benchmark_msgspec import instantiate_msgspec, encode_msgspec, decode_msgspec\n",
    "from data_generator import generate_users_batch\n",
    "from time import perf_counter\n",
    "from typing import Any, Dict, Callable\n",
    "# =======================================================================================\n",
    "#           Dataclass batch operations\n",
    "# =======================================================================================\n",
    "\n",
    "def benchmark_initiate(instantiate_fn: Callable[[dict], Any], users_data: list[dict]) -> tuple[list[Any], float, list[float]]:\n",
    "    times = []\n",
    "    users_output = []\n",
    "    for user_data in users_data:\n",
    "        start_time = perf_counter()\n",
    "        user_instance = instantiate_fn(user_data)\n",
    "        end_time = perf_counter()\n",
    "        times.append(end_time - start_time)\n",
    "        users_output.append(user_instance)\n",
    "    avg_time = sum(times) / len(times)\n",
    "    return users_output, avg_time, times\n",
    "\n",
    "\n",
    "users_data = generate_users_batch(1_000_000)\n",
    "\n",
    "instantiated_dataclasses, average_time_initiate_dataclass, time_initiate_dataclass = benchmark_initiate(instantiate_dataclass, users_data)\n",
    "instantiated_pydantics, average_time_initiate_pydantic, time_initiate_pydantic = benchmark_initiate(instantiate_pydantic, users_data)\n",
    "instantiated_msgspecs, average_time_initiate_msgspec, time_initiate_msgspec = benchmark_initiate(instantiate_msgspec, users_data)\n",
    "\n",
    "print(f\"Dataclass initiation average time over {len(users_data)} samples: {average_time_initiate_dataclass*1e6:.2f} µs\")\n",
    "print(f\"Pydantic initiation average time over {len(users_data)} samples: {average_time_initiate_pydantic*1e6:.2f} µs\")\n",
    "print(f\"Msgspec initiation average time over {len(users_data)} samples: {average_time_initiate_msgspec*1e6:.2f} µs\")\n",
    "# =======================================================================================\n",
    "#           Data generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58ee30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def benchmark(function: Callable, users_data: list, repeats: int = 5) -> float:\n",
    "    times = []\n",
    "    output = None\n",
    "\n",
    "    for i in range(repeats):\n",
    "        start_time = perf_counter()\n",
    "\n",
    "        for user_data in users_data:\n",
    "            if i == repeats - 1:\n",
    "                output = function(user_data)\n",
    "            else:\n",
    "                function(user_data)\n",
    "\n",
    "        function(users_data)\n",
    "\n",
    "        end_time = perf_counter()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "\n",
    "    avg_time = sum(times) / repeats\n",
    "\n",
    "    return avg_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e9c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f723d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e52c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf577a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9ce61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
